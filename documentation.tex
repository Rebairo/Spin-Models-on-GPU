\documentclass{article}

\author{Katharine Hyatt}
\title{Documentation for Lanczos and Hamiltonian generating CUDA code}

\usepackage{fullpage}
\usepackage[usenames, dvipsnames]{color}
\begin{document}
\renewcommand{\int}{\textcolor{Green}{int $\,$}}
\newcommand{\ptrint}{\textcolor{Green}{int $^\ast \,$}}
\newcommand{\const}{\textcolor{Green}{const $\,$}}
\newcommand{\double}{\textcolor{Green}{double $\,$}}
\newcommand{\void}{\textcolor{Green}{void $\,$}}
\newcommand{\typeinttwoptr}{\textcolor{Green}{int2$^\ast \,$}}
\newcommand{\cuDoubleComplex}{\textcolor{Green}{cuDoubleComplex$^\ast \,$}}
\newcommand{\hamstruct}{\textcolor{Green}{hamstruct$^\ast \,$}}
\newcommand{\host}{\textcolor{Red}{$\_\_$host$\_\_ \; \;$}}
\newcommand{\cudaglobal}{\textcolor{Red}{$\_\_$global$\_\_ \; \;$}}
\newcommand{\device}{\textcolor{Red}{$\_\_$device$\_\_ \; \;$}}

\begin{titlepage}
\maketitle
\end{titlepage}

\section{Introduction}

This document describes code for generating Hamiltonians on lattices and then applying the Lanczos procedure to them to approximate the low energy spectrum of the Hamiltonians. There are two main files: hamiltonian.cu, and lanczos.cu. The Hamiltonian generation is treated first. For each major function in the files, the function prototype is given first, then a description of each of its parameters, a description of the steps the function takes and what it actually does, and some possible optimizations that could be implemented. Notes are also included about what other functions would be affected by changes to the function in question.

\section{Hamiltonian Generation}

\subsection{General Description}

There are two codes used to generate the Hamiltonians: \texttt{testhamiltonian.cu} and \texttt{testhamiltonian.h}. 
The ``builder" function, \texttt{ConstructSparseMatrix}, creates and fills a number and three arrays with information about the Hamiltonian for a certain model and number of sites in the COO format

\paragraph{\host \int ConstructSparseMatrix(\int, \int, \ptrint, \cuDoubleComplex, \ptrint, \ptrint) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\int \texttt{model$\_$Type}] A value that tells ConstructSparseMatrix to build a Hamiltonian for the spin 1/2 Heisenberg (1), ..., or ... models.
\item[\int \texttt{lattice$\_$Size}] The number of lattice sites
\item[\ptrint \texttt{Bond}] An array containing the bond information for the model we're using 
\item[\cuDoubleComplex \texttt{hamil$\_$Values}] A device pointer that will have an array of Hamiltonian values allocated at it as \texttt{ConstructSparseMatrix} runs
\item[\ptrint \texttt{hamil$\_$PosRow}] A device pointer that will have an array of row indices allocated at it as \texttt{ConstructSparseMatrix} runs, in the COO form
\item[\ptrint \texttt{hamil$\_$PosCol}] A device pointer that will have an array of column indicies allocated at is as \texttt{ConstructSparseMatrix} runs, in the COO form
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Figuring out (from \texttt{model$\_$Type} and \texttt{lattice$\_$Size}) what the dimension of the system is}
\item{Constructing a basis for the system on the CPU (could maybe move to GPU)}
\item{Allocating device and host FORTRAN style (1D row-major format) arrays to hold the 2D Hamiltonian matrix on host and device}
\item{Call function \texttt{FillSparse} and \texttt{FillDiagonals}, which construct the full Hamiltonian naively and counts the number of nonzero elements per row}
\item{Copying the number of elements per row to a thrust vector, then calling reduce (sum) on them to find how many nonzero elements there are}
\item{Sort the Hamiltonian array on the device by using the Thrust library}
\item{Call function \texttt{FulltoCOO}, which transforms the representation to COO form}
\end{enumerate}

If the function completes successfully, the three \texttt{hamil$\_*$} arrays will be allocated and filled, and can be transformed to CSR format for the Lanczos procedure.
 
Possible Optimizations:
\begin{itemize}
\item{Since memory transfers may be the main bottleneck, it might be faster to construct the basis on the GPU - this way there are two fewer arrays to transfer over}
\item{The sorting is the main bottleneck now. If I only have to sort within a row, then I could probably speed that up a lot.}
\end{itemize}

Other Important Things:
\begin{itemize}
\item{ There's a lot of error checking going on - it's probably possible to outsource this to a function. It uses \texttt{cudaGetErrorString} so people debugging don't have to go do a table lookup.}
\end{itemize} 

\paragraph{\host \void GetBasis(\int , \int , \int , \ptrint , \ptrint ) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\int \texttt{dim}] The full dimension of the Hamiltonian
\item[\int \texttt{lattice$\_$Size}] The number of sites
\item[\int \texttt{Sz}] The spin operator in the $\hat{z}$ direction
\item[\ptrint \texttt{basis$\_$Position}] An empty array of size dim to hold information about the basis positions
\item[\ptrint \texttt{basis}] An empty array of size dim to hold information about the basis
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{For each value from 0 to \texttt{dim}, step through the arrays}
\item{Determine whether or not there is an element at that position}
\end{enumerate}

Possible Optimizations:
\begin{itemize}
\item{If this becomes a device function, you could put temp in shared memory for threads in the y direction}
\item{Using some bithax to get rid of the if statement would probably speed this up a bit}
\end{itemize}

\paragraph{\cudaglobal \void FillSparse(\ptrint , \ptrint , \int, \hamstruct, \int , \int , \const \double ) \\ \\ }
\noindent\textbf{Parameters}:
\begin{description}
\item[\ptrint \texttt{d$\_$basis$\_$Position}] The position information about the basis
\item[\ptrint \texttt{d$\_$basis}] The information about the basis
\item[\int \texttt{dim}] The dimension of the Hamiltonian
\item[\cuDoubleComplex \texttt{H$\_$sort}] An empty array that will store the Hamiltonian positions and values
\item[\int \texttt{d$\_$Bond}] An array that stores the bond information
\item[\int \texttt{lattice$\_$Size}] The number of sites
\item[\const \double \texttt{JJ}] The coupling constant
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Creating some indexing variables, depending where we are in the block/thread structure}
\item{Creating shared memory arrays to store the positions, values, and the intermediate information needed to generate them}
\item{Copy the bond data into shared memory}
\item{Construct the diagonal, off-x, and off-y components of the Hamiltonian in shared memory}
\item{Copy the Hamiltonian information back to global memory}
\end{enumerate}

Possible Optimizations:
\begin{itemize}
\item{Instead of doing the off-x and off-y parts for each lattice site sequentially, I should probably do it by block or warp or something}
\item{I still haven't maxed out the amount of shared memory that each thread can use}
\end{itemize}

Other things:
\begin{itemize}
\item{If you want to add in another model, \texttt{FillSparse} needs to have the enum passed to it, and you'll need to write some equivalents of \texttt{HOffX}, \texttt{HOffY}, and \texttt{HDiagPart}. You might have to change the array size as well}
\end{itemize}

\paragraph{\cudaglobal \void FullToCOO(\int , \hamstruct, \cuDoubleComplex , \ptrint , \ptrint, \int) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\int \texttt{num$\_$Elem}] The number of nonzero elements
\item[\hamstruct \texttt{H$\_$sort}] The array of the Hamiltonian values
\item[\cuDoubleComplex \texttt{hamil$\_$values}] The array that will store only the nonzero Hamiltonian values (for COO form)
\item[\ptrint \texttt{hamil$\_$PosRow}] The array that will store the row positions of the nonzero values (for COO form)
\item[\ptrint \texttt{hamil$\_$PosCol}] The array that will store the column positions of the nonzero values (for COO form)
\item[\int \texttt{dim}] The dimension of the Hamiltonian

\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Copying the sorted Hamiltonian out to the \texttt{hamil$\_$*} arrays}
\end{enumerate}

\subsection{Things Left To Do}
\paragraph{Functionality}
\begin{itemize}
\item{\texttt{FillSparse} might be changed so that it's more intuitive to add/change the models}
\item{More basis-generating functions need to be written}
\end{itemize}
\paragraph{Benchmarking}
\begin{itemize}
\item{Is it faster to do the basis stuff on or off the GPU?}
\item{The sort is the slowest part of this code}
\end{itemize}

\subsection{Changing Things}
Right now the entire process uses double precision. 
Double precision is much slower on GPU than single precision, but this is becoming less pronounced as the Tesla cards get better. 
If you want to change to single precision, you'll need to run a find/replace on \texttt{cuDoubleComplex}, changing it to \texttt{cuComplex}. 
The arrays that use \texttt{cuDoubleComplex} are \texttt{d$\_$H$\_$sort} and \texttt{hamil$\_$values}.\\ 
To add another model, you'll need to write some functions to describe the bond interactions and then just add an if-statement for the model type which will call to them in \texttt{FillSparse}. 
If you've got more than two functions (right now there are only two non-diagonal functions) then just refill the shared memory arrays and push into the global memory \texttt{d$\_$H$\_$sort}. 
\texttt{FillSparse} should be the only function that needs to be changed, because all the others don't call model-specific functions.\\
If you want to use the full Hamiltonian (not just the S$_z$ sector) you'll need to change a few kernel calls because many cards only allow you to call 65336 blocks at a time. 
In this case you'll either need to increase the number of threads per block or call the kernel twice, with each kernel handling part of the data. 
 

\section{Lanczos}

\subsection{Description}
The code, \texttt{lanczos.cu}, takes as input the three COO arrays describing the Hamiltonian and the number of elements, as well as some parameters to control how long the code runs and its output. 
It applies the Lanczos method to the Hamiltonian to reduce it to a tridiagonal form and then diagonalize it using the TQLI method.The Lanczos and eigenvectors are all stored so that expectation values can be found as well.  

\paragraph{\host \void lanczos(\const \int , \const \cuDoubleComplex \&, \const \ptrint \&, \const \ptrint \&, \const \int , \int , \const \int , \const \double )\\ \\ }

\noindent\textbf{Parameters:}
\begin{description}
\item[\const \int \texttt{num$\_$Elem}]{The number of nonzero elements in the Hamiltonian}
\item[\const \cuDoubleComplex \& \texttt{d$\_$H$\_$vals} ]{The array on the GPU that stores the values of the Hamiltonian elements}
\item[\const \ptrint \& \texttt{d$\_$H$\_$rows}]{The array on the GPU that stores the row indices (in COO format) of the Hamiltonian elements}
\item[\const \ptrint \& \texttt{d$\_$H$\_$cols}]{The array on the GPU that stores the column indices of the Hamiltonian elements}
\item[\const \ptrint \texttt{dim}]{The dimension of the Hilbert space}
\item[\int \texttt{max$\_$Iter}]{The maximum number of iterations we want the Lanczos to run for - this is a soft limit, and will be extended if the algorithm isn't converging fast enough}
\item[\const \int \texttt{num$\_$Eig}]{The number of eigenvalues (from the groundstate up) we want to store/print}
\item[\const \double \texttt{conv$\_$req}]{The convergence requirement for the eigenvalues between iterations}
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Initializing a description for the CUSPARSE functions}
\item{Transforming the Hamiltonian representation into the CSR format}
\item{Creating thrust vectors to hold the information about the tridiagonal matrix elements and eigenvectors}
\item{Generating a random starting vector}
\item{Generating the second Lanczos vector and the starting coefficients}
\item{Iterating through the generation of Lanczos vectors (generating the three vectors, finding coefficients, then diagonalizing the Hamiltonian) until the difference between the eigenvalues is less than the convergence requirement}
\item{Printing out the lowest \texttt{num$\_$Eig} eigenvalues}
\end{enumerate}

\noindent\textbf{Possible Optimizations}
\begin{itemize}
\item{Since nearly all of the GPU usage here is handled by CUSPARSE and CUBLAS functions, there's not much kernel tweaking to be done. However, there may be a better way to check convergence and it may be possible to get tqli to run faster}
\end{itemize}

\section{References}

\end{document}
