\documentclass{article}

\author{Katharine Hyatt}
\title{Documentation for Lanczos and Hamiltonian generating CUDA code}

\usepackage{fullpage}
\usepackage[usenames, dvipsnames]{color}
\begin{document}
\renewcommand{\int}{\textcolor{Green}{int $\,$}}
\newcommand{\const}{\textcolor{Green}{const $\,$}}
\renewcommand{\long}{\textcolor{Green}{long $\,$}}
\newcommand{\double}{\textcolor{Green}{double $\,$}}
\newcommand{\typelongptr}{\textcolor{Green}{long$^\ast \,$}}
\newcommand{\void}{\textcolor{Green}{void $\,$}}
\newcommand{\typelongtwoptr}{\textcolor{Green}{long2$^\ast \,$}}
\newcommand{\cuDoubleComplex}{\textcolor{Green}{cuDoubleComplex$^\ast \,$}}
\newcommand{\hamstruct}{\textcolor{Green}{hamstruct$^\ast \,$}}
\newcommand{\host}{\textcolor{Red}{$\_\_$host$\_\_ \; \;$}}
\newcommand{\cudaglobal}{\textcolor{Red}{$\_\_$global$\_\_ \; \;$}}
\newcommand{\device}{\textcolor{Red}{$\_\_$device$\_\_ \; \;$}}

\begin{titlepage}
\maketitle
\end{titlepage}

\section{Introduction}

This document describes code for generating Hamiltonians on lattices and then applying the Lanczos procedure to them to approximate the low energy spectrum of the Hamiltonians. There are two main files: hamiltonian.cu, and lanczos.cu. The Hamiltonian generation is treated first. For each major function in the files, the function prototype is given first, then a description of each of its parameters, a description of the steps the function takes and what it actually does, and some possible optimizations that could be implemented. Notes are also included about what other functions would be affected by changes to the function in question.

\section{Hamiltonian Generation}

\subsection{General Description}

There are two codes used to generate the Hamiltonians: hamiltonian.cu and hamiltonian.h. The ``builder" function, ConstructSparseMatrix, creates and fills a number and three arrays with information about the Hamiltonian for a certain model and number of sites in the COO format

\paragraph{\host \int ConstructSparseMatrix(\int, \int, \typelongptr, \cuDoubleComplex, \typelongptr, \typelongptr) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\int model$\_$Type] A value that tells ConstructSparseMatrix to build a Hamiltonian for the spin 1/2 Heisenberg (1), ..., or ... models.
\item[\int lattice$\_$Size] The number of lattice sites
\item[\typelongptr Bond] An array containing the bond information for the model we're using 
\item[\cuDoubleComplex hamil$\_$Values] A device pointer that will have an array of Hamiltonian values allocated at it as ConstructSparseMatrix runs
\item[\typelongptr hamil$\_$PosRow] A device pointer that will have an array of row indices allocated at it as ConstructSparseMatrix runs, in the COO form
\item[\typelongptr hamil$\_$PosCol] A device pointer that will have an array of column indicies allocated at is as ConstructSparseMatrix runs, in the COO form
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Figuring out (from model$\_$Type and lattice$\_$Size) what the dimension of the system is}
\item{Constructing a basis for the system on the CPU (could maybe move to GPU)}
\item{Allocating device and host FORTRAN style (1D row-major format) arrays to hold the 2D Hamiltonian matrix on host and device}
\item{Call function FillSparse, which constructs the Hamiltonian naively and counts the number of nonzero elements per row}
\item{Call function GetNumElem, which finds how many nonzero elements there are}
\item{Call function CompressSparse, which moves all the nonzero elements and their positions into the array d$\_$H$\_$sort which is an array of hamstructs}
\item{Sort the Hamiltonian array on the device by using the Thrust library}
\item{Call function FulltoCOO, which transforms the representation to COO form}
\end{enumerate}

If the function completes successfully, the three hamil$\_*$ arrays will be allocated and filled, and can be transformed to CSR format for the Lanczos procedure.
 
Possible Optimizations:
\begin{itemize}
\item{Since memory transfers may be the main bottleneck, it might be faster to construct the basis on the GPU - this way there are two fewer arrays to transfer over}
\end{itemize}

Other Important Things:
\begin{itemize}
\item{ There's a lot of error checking going on - it's probably possible to outsource this to a function. It uses cudaGetErrorString so people debugging don't have to go do a table lookup.}
\end{itemize} 

\paragraph{\host \void GetBasis(\int , \int , \int , \long [], \long []) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\long dim] The full dimension of the Hamiltonian
\item[\int lattice$\_$Size] The number of sites
\item[\int Sz] The spin operator in the $\hat{z}$ direction
\item[\long basis$\_$Position] An empty array of size dim to hold information about the basis positions
\item[\long basis] An empty array of size dim to hold information about the basis
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{For each value from 0 to dim, step through the arrays}
\item{Determine whether or not there is an element at that position}
\end{enumerate}

Possible Optimizations:
\begin{itemize}
\item{If this becomes a device function, you could put temp in shared memory for threads in the y direction}
\item{Using some bithax to get rid of the if statement would probably speed this up a bit}
\end{itemize}

\paragraph{\cudaglobal \void FillSparse(\typelongptr , \typelongptr , \int, \cuDoubleComplex , \typelongtwoptr , \int , \int , \const \double ) \\ \\ }
\noindent\textbf{Parameters}:
\begin{description}
\item[\typelongptr d$\_$basis$\_$Position] The position information about the basis
\item[\typelongptr d$\_$basis] The information about the basis
\item[\long dim] The dimension of the Hamiltonian
\item[\cuDoubleComplex H$\_$vals] An empty array that will store the Hamiltonian values
\item[\typelongtwoptr H$\_$pos] An empty array that will store the Hamiltonian positions
\item[\int d$\_$Bond] An array that stores the bond information
\item[\int lattice$\_$Size] The number of sites
\item[const \double JJ] The coupling constant
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Create some indexing variables, depending where we are in the block/thread structure}
\item{Create several shared arrays to speed up memory access}
\item{Copy the bond data into shared memory}
\item{Construct the diagonal, off-x, and off-y components of the Hamiltonian in shared memory}
\item{Copy the Hamiltonian information back to global memory}
\end{enumerate}

Possible Optimizations:
\begin{itemize}
\item{Instead of doing the off-x and off-y parts for each lattice site sequentially, I should probably do it by block or something}
\item{There's probably a way to use shared memory more intelligently here}
\end{itemize}

Other things:
\begin{itemize}
\item{H$\_$pos is a typelongptr because this makes later functions easier. The first typelongptr is the row index and the second is the column.}
\item{If you want to add in another model, FillSparse needs to have the enum passed to it, and you'll need to write some equivalents of HOffX, HOffY, and HDiagPart. You might have to change the array size as well}
\end{itemize}

\paragraph{\cudaglobal \void GetNumElem( \typelongtwoptr , \int ) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\typelongtwoptr H$\_$pos] The array of Hamiltonian positions
\item[\int lattice$\_$Size] The number of lattice sites
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Each thread gets the number of elements in the corresponding row}
\item{This number is added to a global $\_\_$device$\_\_$ variable called d$\_$num$\_$Elem}
\end{enumerate}

\paragraph{\cudaglobal \void CompressSparse(\cuDoubleComplex , \typelongtwoptr , \hamstruct , \int , \const \int ) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\cuDoubleComplex H$\_$vals] The array of Hamiltonian values
\item[\typelongtwoptr H$\_$pos] The array of Hamiltonian positions in the columns
\item[\hamstruct H$\_$sort] The array of ``good'' values to be sorted
\item[\long dim] The dimension of the Hamiltonian
\item[\int lattice$\_$Size] The number of sites
\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Checks to see if the column index of a value is -1}
\item{If not, the value is put into the sorting array. If yes, nothing is done}
\item{The sorting array lacks information about how many ``good'' values per row there are, but this doesn't matter because the struct overloading of > and < ensures that row-ordering is preserved }
\end{enumerate}

Possible Optimizations:
\begin{itemize}
\item{If CUDA stores TRUE/FALSE as -1/0, that could make this much easier}
\end{itemize}


\paragraph{\cudaglobal \void FullToCOO(\int , \cuDoubleComplex , \typelongtwoptr , \cuDoubleComplex , \typelongptr , \typelongptr, \long) \\ \\}
\noindent\textbf{Parameters}:
\begin{description}
\item[\int num$\_$Elem] The number of nonzero elements
\item[\cuDoubleComplex H$\_$vals] The array of the Hamiltonian values
\item[\typelongtwoptr H$\_$pos] The array of the Hamiltonian positions
\item[\cuDoubleComplex hamil$\_$values] The array that will store only the nonzero Hamiltonian values (for COO form)
\item[\typelongptr hamil$\_$PosRow] The array that will store the row positions of the nonzero values (for COO form)
\item[\typelongptr hamil$\_$PosCol] The array that will store the column positions of the nonzero values (for COO form)
\item[\long dim] The dimension of the Hamiltonian

\end{description}

\noindent\textbf{The function proceeds by:}
\begin{enumerate}
\item{Loading the Hamiltonian information into shared memory}
\item{Copying it back out to the hamil$\_$* arrays}
\end{enumerate}

\subsection{Things Left To Do}
\paragraph{Functionality}
\begin{itemize}
\item{FillSparse might be changed so that it's more intuitive to add/change the models}
\item{More basis-generating functions need to be written}
\end{itemize}
\paragraph{Benchmarking}
\begin{itemize}
\item{Is it faster to do the basis stuff on or off the GPU?}
\item{CompressSparse and the sort are the things slowing this down.}
\item{It may be worthwhile to use asynchronous memory transfers in a couple places because the kernel isn't operating on the set of data that needs to move}
\end{itemize}

\subsection{Changing Things}
Right now the entire process uses double precision. Double precision is much slower on GPU than single precision, but this is becoming less pronounced as the Tesla cards get better. If you want to change to single precision, you'll need to run a find/replace on cuDoubleComplex, changing it to cuFloatComplex. The arrays that use cuDoubleComplex are *$\_$H$\_$vals and hamil$\_$values.\\ 
To add another model, you'll need to write some functions to describe the bond interactions and then just add an if-statement for the model type which will call to them in FillSparse. If you've got more than two functions (right now there are only two non-diagonal functions) then just refill the shared memory arrays and push into the global memory d$\_$H$\_$*s. FillSparse should be the only function that needs to be changed, because all the others don't call model-specific functions.\\
If you want to use the full Hamiltonian (not just the S$_z$ sector) you'll need to change a few kernel calls because many cards only allow you to call 65336 blocks at a time. In this case you'll either need to increase the number of threads per block or call the kernel twice, with each kernel handling part of the data. 
 

\section{Lanczos}

\section{References}

\end{document}
